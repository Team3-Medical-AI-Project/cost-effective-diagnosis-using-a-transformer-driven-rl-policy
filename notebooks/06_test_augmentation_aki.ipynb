{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8c6df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Splitting Complete ---\n",
      "Original Training Set Class Distribution:\n",
      "kdigo_aki\n",
      "1    14310\n",
      "0     4797\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- 2. Scaling and Imputation Complete ---\n",
      "\n",
      "--- 3. Augmenting Training Data ---\n",
      "Original class distribution: {1: 14310, 0: 4797}\n",
      "Majority class: 1 (count: 14310)\n",
      "Minority class: 0 (count: 4797)\n",
      "Generating 4756 samples with SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning:\n",
      "\n",
      "`BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CTGAN on 4797 original minority samples to generate 4757 new samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sdv\\single_table\\base.py:163: FutureWarning:\n",
      "\n",
      "The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "\n",
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sdv\\single_table\\base.py:129: UserWarning:\n",
      "\n",
      "We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AKI Data Augmentation Test (v1.2 - Final Fix)\n",
    "\n",
    "This notebook tests the full data preparation pipeline on the AKI dataset.\n",
    "This version contains a more robust data augmentation logic to fix the\n",
    "recurring SMOTE ValueError.\n",
    "\n",
    "The final output is a verification of the balanced dataset and a bar chart\n",
    "visualizing the class balance before and after augmentation.\n",
    "\"\"\"\n",
    "# --- 1. Setup ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "\n",
    "# --- 2. Configuration for AKI ---\n",
    "CONFIG = {\n",
    "    \"INPUT_FILE\": \"../data/preprocessed/aki_feature_matrix.csv\",\n",
    "    \"OUTPUT_DIR\": Path(\"../data/processed/aki/\"),\n",
    "    \"TARGET_COLUMN\": \"kdigo_aki\",\n",
    "    \"ID_COLUMNS\": ['subject_id', 'hadm_id', 'stay_id'],\n",
    "    \"RANDOM_STATE\": 42\n",
    "}\n",
    "\n",
    "# --- 3. The Main Pipeline Functions ---\n",
    "\n",
    "def process_and_augment_data(config):\n",
    "    \"\"\"\n",
    "    Runs the entire pipeline: load, split, scale, impute, and augment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(config[\"INPUT_FILE\"])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{config['INPUT_FILE']}'\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Splitting ---\n",
    "    X = df.drop(columns=[config[\"TARGET_COLUMN\"]])\n",
    "    y = df[config[\"TARGET_COLUMN\"]]\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=config[\"RANDOM_STATE\"], stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=config[\"RANDOM_STATE\"], stratify=y_temp)\n",
    "    \n",
    "    original_distribution = y_train.value_counts(normalize=True)\n",
    "    print(\"--- 1. Data Splitting Complete ---\")\n",
    "    print(f\"Original Training Set Class Distribution:\\n{y_train.value_counts()}\")\n",
    "\n",
    "    # --- Scaling and Imputation ---\n",
    "    id_cols = [col for col in config[\"ID_COLUMNS\"] if col in X_train.columns]\n",
    "    feature_cols = [col for col in X_train.columns if col not in id_cols]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    \n",
    "    X_train_processed = X_train.copy()\n",
    "    X_train_processed[feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "    X_train_processed[feature_cols] = imputer.fit_transform(X_train_processed[feature_cols])\n",
    "    print(\"\\n--- 2. Scaling and Imputation Complete ---\")\n",
    "\n",
    "    # --- Augmentation ---\n",
    "    X_train_aug, y_train_aug = augment_training_data(X_train_processed, y_train, config)\n",
    "    \n",
    "    augmented_distribution = y_train_aug.value_counts(normalize=True)\n",
    "    print(f\"\\nFinal Augmented Training Set Class Distribution:\\n{y_train_aug.value_counts()}\")\n",
    "    \n",
    "    return original_distribution, augmented_distribution\n",
    "\n",
    "def augment_training_data(X_train_imputed, y_train, config):\n",
    "    \"\"\"\n",
    "    Applies the hybrid SMOTE and CTGAN augmentation strategy using a more robust logic.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 3. Augmenting Training Data ---\")\n",
    "    \n",
    "    id_cols = [col for col in config[\"ID_COLUMNS\"] if col in X_train_imputed.columns]\n",
    "    X_train_features = X_train_imputed.drop(columns=id_cols)\n",
    "    \n",
    "    # Get original class counts and identify majority/minority classes\n",
    "    original_counts = y_train.value_counts()\n",
    "    print(f\"Original class distribution: {original_counts.to_dict()}\")\n",
    "    \n",
    "    # Identify majority and minority classes\n",
    "    if original_counts[0] > original_counts[1]:\n",
    "        majority_class = 0\n",
    "        minority_class = 1\n",
    "        n_majority = original_counts[0]\n",
    "        n_minority = original_counts[1]\n",
    "    else:\n",
    "        majority_class = 1\n",
    "        minority_class = 0\n",
    "        n_majority = original_counts[1]\n",
    "        n_minority = original_counts[0]\n",
    "    \n",
    "    print(f\"Majority class: {majority_class} (count: {n_majority})\")\n",
    "    print(f\"Minority class: {minority_class} (count: {n_minority})\")\n",
    "    \n",
    "    if n_minority == 0:\n",
    "        print(\"No minority samples found, skipping augmentation.\")\n",
    "        return X_train_imputed, y_train\n",
    "        \n",
    "    num_to_generate = n_majority - n_minority\n",
    "    num_from_smote = num_to_generate // 2\n",
    "    num_from_gan = num_to_generate - num_from_smote\n",
    "    \n",
    "    # --- SMOTE ---\n",
    "    print(f\"Generating {num_from_smote} samples with SMOTE...\")\n",
    "    smote_strategy = {minority_class: n_minority + num_from_smote}\n",
    "    smote = SMOTE(sampling_strategy=smote_strategy, random_state=config[\"RANDOM_STATE\"])\n",
    "    X_smote_interim, y_smote_interim = smote.fit_resample(X_train_features, y_train)\n",
    "    \n",
    "    # --- CTGAN ---\n",
    "    original_minority_features = X_train_features[y_train == minority_class]\n",
    "    print(f\"Training CTGAN on {len(original_minority_features)} original minority samples to generate {num_from_gan} new samples...\")\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=original_minority_features)\n",
    "    ctgan = CTGANSynthesizer(metadata, epochs=300, verbose=False)\n",
    "    ctgan.fit(original_minority_features)\n",
    "    gan_generated_features = ctgan.sample(num_rows=num_from_gan)\n",
    "    gan_generated_labels = pd.Series([minority_class] * num_from_gan, name=config[\"TARGET_COLUMN\"])\n",
    "    \n",
    "    # --- Combine ---\n",
    "    print(\"\\nCombining original, SMOTE-augmented, and GAN-generated data...\")\n",
    "    X_smote_interim_df = pd.DataFrame(X_smote_interim, columns=X_train_features.columns)\n",
    "    y_smote_interim_s = pd.Series(y_smote_interim, name=config[\"TARGET_COLUMN\"])\n",
    "    \n",
    "    X_final = pd.concat([X_smote_interim_df, gan_generated_features], ignore_index=True)\n",
    "    y_final = pd.concat([y_smote_interim_s, gan_generated_labels], ignore_index=True)\n",
    "    \n",
    "    # Shuffle the final combined data\n",
    "    shuffled_indices = X_final.sample(frac=1, random_state=config[\"RANDOM_STATE\"]).index\n",
    "    X_train_aug = X_final.loc[shuffled_indices].reset_index(drop=True)\n",
    "    y_train_aug = y_final.loc[shuffled_indices].reset_index(drop=True)\n",
    "    \n",
    "    return X_train_aug, y_train_aug\n",
    "\n",
    "# --- 4. Run Pipeline and Get Distributions ---\n",
    "original_dist, augmented_dist = process_and_augment_data(CONFIG)\n",
    "\n",
    "# --- 5. Visualize Results for Proof ---\n",
    "if original_dist is not None:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "    fig.suptitle('Class Distribution Before and After Hybrid Augmentation (AKI)', fontsize=16)\n",
    "\n",
    "    # Before\n",
    "    sns.barplot(x=original_dist.index, y=original_dist.values, ax=axes[0], palette='pastel')\n",
    "    axes[0].set_title('Before Augmentation')\n",
    "    axes[0].set_xlabel('KDIGO AKI Flag')\n",
    "    axes[0].set_ylabel('Proportion')\n",
    "    axes[0].set_xticklabels(['No AKI (0)', 'AKI (1)'])\n",
    "\n",
    "    # After\n",
    "    sns.barplot(x=augmented_dist.index, y=augmented_dist.values, ax=axes[1], palette='pastel')\n",
    "    axes[1].set_title('After SMOTE + CTGAN Augmentation')\n",
    "    axes[1].set_xlabel('KDIGO AKI Flag')\n",
    "    axes[1].set_xticklabels(['No AKI (0)', 'AKI (1)'])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_med_diag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
