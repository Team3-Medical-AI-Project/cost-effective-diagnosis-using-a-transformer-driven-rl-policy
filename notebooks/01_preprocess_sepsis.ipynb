{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyI2qGWIgIjS",
        "outputId": "3c6e6e8a-5c24-43f3-e0a7-ca9367072bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doL-l9qwgfd4",
        "outputId": "3278a854-dfd0-465e-bd4c-3cbd3159b8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading cohort...\n",
            "Identified 11321 unique patients and 61530 sepsis-related admissions.\n",
            "Linking cohort to ICU stays...\n",
            "Extracting vitals from chartevents (this may take a few minutes)...\n",
            "Extracting labs from labevents (this may take a few minutes)...\n",
            "Assembling final feature matrix...\n",
            "\n",
            "--- Preprocessing Complete ---\n",
            "Final matrix shape: (16780, 40)\n",
            "Saved to: data\\preprocessed\\sepsis_feature_matrix.csv\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "MIMIC-IV Sepsis Cohort Preprocessing (for uncompressed .csv files)\n",
        "\n",
        "Description:\n",
        "This script executes Phase 1 of the data processing pipeline. It builds the\n",
        "Sepsis cohort from the MIMIC-IV dataset, extracts relevant features from the\n",
        "first 24 hours of each patient's ICU stay, and generates a final feature matrix.\n",
        "\n",
        "Output:\n",
        "- 'sepsis_feature_matrix.csv': A CSV file where each row corresponds to a unique\n",
        "  ICU stay, containing static features, aggregated vitals, and lab values.\n",
        "  Missing values are represented as NaN, ready for imputation in the next phase.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- Constants & Configuration ---\n",
        "\n",
        "# Paths to the MIMIC-IV data directories ON GOOGLE DRIVE.\n",
        "# !!! ENSURE THESE PATHS ARE CORRECT FOR YOUR DRIVE SETUP !!!\n",
        "DRIVE_BASE_PATH = \"D:\\mimic-iv-3.1\\mimic-iv-3.1\"\n",
        "HOSP_DATA_PATH = Path(DRIVE_BASE_PATH + \"\\csv files\\hosp\")\n",
        "ICU_DATA_PATH = Path(DRIVE_BASE_PATH + \"\\csv files\\icu\")\n",
        "\n",
        "# Define where the output file will be saved in your Drive.\n",
        "OUTPUT_PATH = Path(\"../data/preprocessed/\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Time window for feature extraction from ICU admission.\n",
        "TIME_WINDOW_HOURS = 24\n",
        "\n",
        "# Clinical variables defined from the project's data analysis phase.\n",
        "SEPSIS_ICD_CODES = [\n",
        "    'A021', 'A227', 'A267', 'A327', 'A400', 'A401', 'A403', 'A408', 'A409',\n",
        "    'A4101', 'A4102', 'A411', 'A412', 'A413', 'A414', 'A4150', 'A4151',\n",
        "    'A4152', 'A4153', 'A4159', 'A4181', 'A4189', 'A419', 'A427', 'A5486',\n",
        "    'B377', 'O0337', 'O0387', 'O0487', 'O0737', 'O0882', 'O85', 'O8604',\n",
        "    'P360', 'P3610', 'P3619', 'P362', 'P3630', 'P3639', 'P364', 'P365',\n",
        "    'P368', 'P369', 'R6520', 'R6521', 'T8144', 'T8144XA', 'T8144XD', 'T8144XS'\n",
        "]\n",
        "\n",
        "LAB_PANEL_ITEMIDS = {\n",
        "    'cbc_hematocrit': 51221, 'cbc_hemoglobin': 51222, 'cbc_platelet': 51265,\n",
        "    'cbc_rbc': 51279, 'cbc_wbc': 51301, 'cmp_bicarbonate': 50882,\n",
        "    'cmp_creatinine': 50912, 'cmp_glucose': 50931, 'cmp_potassium': 50971,\n",
        "    'cmp_bun': 51006, 'cmp_aniongap': 50868, 'cmp_lactate': 50813,\n",
        "    'abg_ph': 50820, 'abg_o2_saturation': 50817, 'abg_base_excess': 50802,\n",
        "    'aptt_ptt': 51275, 'aptt_inr': 51237\n",
        "}\n",
        "\n",
        "VITAL_SIGN_ITEMIDS = {\n",
        "    'heart_rate': 220045, 'sbp': 220179, 'dbp': 220180,\n",
        "    'respiratory_rate': 220210, 'temperature_c': 223761, 'spo2': 220277\n",
        "}\n",
        "\n",
        "# Invert dictionaries for easy name lookup\n",
        "ITEMID_TO_LAB_NAME = {v: k for k, v in LAB_PANEL_ITEMIDS.items()}\n",
        "ITEMID_TO_VITAL_NAME = {v: k for k, v in VITAL_SIGN_ITEMIDS.items()}\n",
        "\n",
        "\n",
        "def load_sepsis_cohort(hosp_path):\n",
        "    \"\"\"\n",
        "    Identifies the Sepsis cohort using ICD-10 codes and merges demographic\n",
        "    and admission data to create a base cohort table.\n",
        "    \"\"\"\n",
        "    print(\"Loading cohort...\")\n",
        "    diagnoses = pd.read_csv(hosp_path / 'diagnoses_icd.csv')\n",
        "    sepsis_subjects = diagnoses[diagnoses['icd_code'].isin(SEPSIS_ICD_CODES)]['subject_id'].unique()\n",
        "\n",
        "    patients = pd.read_csv(hosp_path / 'patients.csv', usecols=['subject_id', 'gender', 'anchor_age', 'anchor_year'])\n",
        "    admissions = pd.read_csv(hosp_path / 'admissions.csv', usecols=['subject_id', 'hadm_id', 'admittime', 'hospital_expire_flag'])\n",
        "    admissions['admittime'] = pd.to_datetime(admissions['admittime'])\n",
        "\n",
        "    # Filter for sepsis patients and calculate age at admission\n",
        "    sepsis_admissions = admissions[admissions['subject_id'].isin(sepsis_subjects)].copy()\n",
        "    sepsis_admissions = pd.merge(sepsis_admissions, patients, on='subject_id')\n",
        "    sepsis_admissions['age'] = sepsis_admissions['anchor_age'] + (sepsis_admissions['admittime'].dt.year - sepsis_admissions['anchor_year'])\n",
        "\n",
        "    print(f\"Identified {len(sepsis_subjects)} unique patients and {len(sepsis_admissions)} sepsis-related admissions.\")\n",
        "    return sepsis_admissions\n",
        "\n",
        "\n",
        "def link_cohort_to_icu_stays(cohort_df, icu_path):\n",
        "    \"\"\"\n",
        "    Links hospital admissions to ICU stays and defines the 24-hour feature\n",
        "    extraction window for each stay.\n",
        "    \"\"\"\n",
        "    print(\"Linking cohort to ICU stays...\")\n",
        "    icustays = pd.read_csv(icu_path / 'icustays.csv', usecols=['hadm_id', 'stay_id', 'intime'])\n",
        "    icustays['intime'] = pd.to_datetime(icustays['intime'])\n",
        "\n",
        "    cohort_icu = pd.merge(cohort_df, icustays, on='hadm_id')\n",
        "    cohort_icu['endtime'] = cohort_icu['intime'] + timedelta(hours=TIME_WINDOW_HOURS)\n",
        "\n",
        "    return cohort_icu.drop_duplicates(subset=['hadm_id', 'stay_id'])\n",
        "\n",
        "\n",
        "def extract_vitals_for_stays(stay_windows_df, icu_path):\n",
        "    \"\"\"\n",
        "    Extracts and aggregates vital signs from chartevents for the given ICU stays.\n",
        "    \"\"\"\n",
        "    print(\"Extracting vitals from chartevents (this may take a few minutes)...\")\n",
        "    vitals_all = []\n",
        "    # Process the large chartevents file in chunks\n",
        "    for chunk in pd.read_csv(icu_path / 'chartevents.csv',\n",
        "                             usecols=['stay_id', 'itemid', 'charttime', 'valuenum'],\n",
        "                             chunksize=10_000_000, low_memory=False):\n",
        "\n",
        "        chunk.dropna(subset=['valuenum'], inplace=True)\n",
        "        chunk = chunk[chunk['stay_id'].isin(stay_windows_df['stay_id'])]\n",
        "        chunk = chunk[chunk['itemid'].isin(VITAL_SIGN_ITEMIDS.values())]\n",
        "\n",
        "        if not chunk.empty:\n",
        "            chunk['charttime'] = pd.to_datetime(chunk['charttime'])\n",
        "            chunk_merged = pd.merge(chunk, stay_windows_df, on='stay_id', how='left')\n",
        "            vitals_in_window = chunk_merged[\n",
        "                (chunk_merged['charttime'] >= chunk_merged['intime']) &\n",
        "                (chunk_merged['charttime'] <= chunk_merged['endtime'])\n",
        "            ]\n",
        "            vitals_all.append(vitals_in_window[['stay_id', 'itemid', 'valuenum']])\n",
        "\n",
        "    vitals_df = pd.concat(vitals_all)\n",
        "\n",
        "    # Aggregate vitals to get mean, min, and max over the window\n",
        "    vitals_agg = vitals_df.groupby(['stay_id', 'itemid'])['valuenum'].agg(['mean', 'min', 'max']).unstack()\n",
        "    vitals_agg.columns = [f\"{ITEMID_TO_VITAL_NAME[itemid]}_{stat}\" for stat, itemid in vitals_agg.columns]\n",
        "\n",
        "    return vitals_agg\n",
        "\n",
        "\n",
        "def extract_labs_for_stays(stay_windows_df, hosp_path):\n",
        "    \"\"\"\n",
        "    Extracts and aggregates lab measurements from labevents for the given ICU stays.\n",
        "    \"\"\"\n",
        "    print(\"Extracting labs from labevents (this may take a few minutes)...\")\n",
        "    relevant_hadm_ids = stay_windows_df['hadm_id'].unique()\n",
        "    labs_all = []\n",
        "\n",
        "    # Process the large labevents file in chunks\n",
        "    for chunk in pd.read_csv(hosp_path / 'labevents.csv',\n",
        "                             usecols=['hadm_id', 'itemid', 'charttime', 'valuenum'],\n",
        "                             chunksize=10_000_000, low_memory=False):\n",
        "\n",
        "        chunk.dropna(subset=['valuenum'], inplace=True)\n",
        "        chunk = chunk[chunk['hadm_id'].isin(relevant_hadm_ids)]\n",
        "        chunk = chunk[chunk['itemid'].isin(LAB_PANEL_ITEMIDS.values())]\n",
        "\n",
        "        if not chunk.empty:\n",
        "            chunk['charttime'] = pd.to_datetime(chunk['charttime'])\n",
        "            chunk_merged = pd.merge(chunk, stay_windows_df, on='hadm_id', how='left')\n",
        "            labs_in_window = chunk_merged[\n",
        "                (chunk_merged['charttime'] >= chunk_merged['intime']) &\n",
        "                (chunk_merged['charttime'] <= chunk_merged['endtime'])\n",
        "            ]\n",
        "            labs_all.append(labs_in_window[['stay_id', 'itemid', 'valuenum']])\n",
        "\n",
        "    labs_df = pd.concat(labs_all)\n",
        "\n",
        "    # Aggregate labs to get the mean value over the window\n",
        "    labs_agg = labs_df.groupby(['stay_id', 'itemid'])['valuenum'].mean().unstack()\n",
        "    labs_agg.columns = [ITEMID_TO_LAB_NAME[itemid] for itemid in labs_agg.columns]\n",
        "\n",
        "    return labs_agg\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline.\"\"\"\n",
        "    OUTPUT_PATH.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # 1. Identify cohort and get static data\n",
        "    sepsis_admissions = load_sepsis_cohort(HOSP_DATA_PATH)\n",
        "\n",
        "    # 2. Link to ICU stays to define time windows\n",
        "    sepsis_icu = link_cohort_to_icu_stays(sepsis_admissions, ICU_DATA_PATH)\n",
        "    stay_windows = sepsis_icu[['stay_id', 'hadm_id', 'intime', 'endtime']].copy()\n",
        "\n",
        "    # 3. Extract and aggregate time-series features\n",
        "    vitals_features = extract_vitals_for_stays(stay_windows, ICU_DATA_PATH)\n",
        "    labs_features = extract_labs_for_stays(stay_windows, HOSP_DATA_PATH)\n",
        "\n",
        "    # 4. Assemble the final feature matrix\n",
        "    print(\"Assembling final feature matrix...\")\n",
        "    base_data = sepsis_icu.set_index('stay_id')\n",
        "    final_matrix = base_data.join(vitals_features).join(labs_features)\n",
        "\n",
        "    # 5. Clean up and select final columns\n",
        "    final_matrix['gender'] = final_matrix['gender'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "\n",
        "    static_cols = ['subject_id', 'hadm_id', 'age', 'gender', 'hospital_expire_flag']\n",
        "    feature_cols = list(vitals_features.columns) + list(labs_features.columns)\n",
        "    # Ensure all expected feature columns are present, filling with NaN if not\n",
        "    for col in feature_cols:\n",
        "        if col not in final_matrix:\n",
        "            final_matrix[col] = np.nan\n",
        "\n",
        "    final_matrix = final_matrix[static_cols + feature_cols]\n",
        "\n",
        "    # 6. Save the output\n",
        "    output_file = OUTPUT_PATH / 'sepsis_feature_matrix.csv'\n",
        "    final_matrix.to_csv(output_file)\n",
        "\n",
        "    print(\"\\n--- Preprocessing Complete ---\")\n",
        "    print(f\"Final matrix shape: {final_matrix.shape}\")\n",
        "    print(f\"Saved to: {output_file}\")\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM95YZEjDmdAAZltQPa93Ur",
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
