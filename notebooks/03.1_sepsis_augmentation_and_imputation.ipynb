{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cdd5787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Splitting, Scaling, and Imputing Data ---\n",
      "Scaling complete.\n",
      "Imputation complete.\n",
      "\n",
      "--- 2. Augmenting Training Data ---\n",
      "\n",
      "Applying SMOTE...\n",
      "Detecting metadata for CTGAN...\n",
      "Training CTGAN on original minority data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:66: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.07684979  0.07684979 -1.87512084 ... -0.11834727 -0.11834727\n",
      " -1.87512084]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:66: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.89783048 -1.11379601  0.89783048 ...  0.89783048 -1.11379601\n",
      " -1.11379601]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:67: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.89913552 -0.37861002  1.18296648 ...  0.98776941  0.46724391\n",
      "  0.59737529]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:67: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.89783048 -1.11379601 -1.11379601 ...  0.89783048  0.89783048\n",
      "  0.89783048]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:68: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.83406984  0.66244098 -0.76900415 ... -0.18341296  0.59737529\n",
      " -1.22446396]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39500\\1773278307.py:68: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.89783048 0.89783048 0.89783048 ... 0.89783048 0.89783048 0.89783048]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning:\n",
      "\n",
      "`BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "\n",
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sdv\\single_table\\base.py:163: FutureWarning:\n",
      "\n",
      "The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "\n",
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sdv\\single_table\\base.py:129: UserWarning:\n",
      "\n",
      "We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining original and synthetic data...\n",
      "\n",
      "Final augmented training set distribution:\n",
      "hospital_expire_flag\n",
      "1.0    0.5\n",
      "0.0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- 3. Saving Artifacts ---\n",
      "Processed data saved to 'C:\\Users\\vamsi\\cost-effective-diagnosis-using-a-transformer-driven-rl-policy\\data\\processed'\n",
      "\n",
      "Phase 2 processing complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 2: Data Splitting, Hybrid Augmentation, and Preparation for Imputation\n",
    "\n",
    "Description:\n",
    "This script takes the preprocessed feature matrix from Phase 1 and prepares it \n",
    "for model training. It performs three critical steps:\n",
    "1. Splits the data into training, validation, and test sets.\n",
    "2. Applies feature scaling (StandardScaler) to normalize the data.\n",
    "3. Balances the training set using a hybrid SMOTE and CTGAN strategy.\n",
    "\n",
    "Outputs:\n",
    "- train_X.csv, train_y.csv: The scaled and balanced training data.\n",
    "- val_X.csv, val_y.csv: The scaled validation data.\n",
    "- test_X.csv, test_y.csv: The scaled test data.\n",
    "- scaler.joblib: The saved scaler object, crucial for consistent scaling.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "# Updated to match the paths you are using\n",
    "CONFIG = {\n",
    "    \"INPUT_FILE\": \"../data/preprocessed/sepsis_feature_matrix.csv\",\n",
    "    \"OUTPUT_DIR\": Path(\"../data/processed/\"),\n",
    "    \"TARGET_COLUMN\": \"hospital_expire_flag\",\n",
    "    \"ID_COLUMNS\": ['subject_id', 'hadm_id', 'stay_id'],\n",
    "    \"TEST_SIZE\": 0.2,\n",
    "    \"VALIDATION_SIZE\": 0.1,\n",
    "    \"RANDOM_STATE\": 42\n",
    "}\n",
    "\n",
    "def split_scale_impute_data(df, config):\n",
    "    \"\"\"\n",
    "    Loads, splits, scales, and imputes the data.\n",
    "    The scaler and imputer are fit ONLY on the training data and saved.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Splitting, Scaling, and Imputing Data ---\")\n",
    "    \n",
    "    X = df.drop(columns=[config[\"TARGET_COLUMN\"]])\n",
    "    y = df[config[\"TARGET_COLUMN\"]]\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"], stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=config[\"VALIDATION_SIZE\"], random_state=config[\"RANDOM_STATE\"], stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    id_cols = [col for col in config[\"ID_COLUMNS\"] if col in X_train.columns]\n",
    "    feature_cols = [col for col in X_train.columns if col not in id_cols]\n",
    "    \n",
    "    # --- Feature Scaling ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled.loc[:, feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "    X_val_scaled.loc[:, feature_cols] = scaler.transform(X_val[feature_cols])\n",
    "    X_test_scaled.loc[:, feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "    print(\"Scaling complete.\")\n",
    "    \n",
    "    # --- Imputation ---\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    \n",
    "    X_train_imputed = X_train_scaled.copy()\n",
    "    X_val_imputed = X_val_scaled.copy()\n",
    "    X_test_imputed = X_test_scaled.copy()\n",
    "    \n",
    "    X_train_imputed.loc[:, feature_cols] = imputer.fit_transform(X_train_scaled[feature_cols])\n",
    "    X_val_imputed.loc[:, feature_cols] = imputer.transform(X_val_scaled[feature_cols])\n",
    "    X_test_imputed.loc[:, feature_cols] = imputer.transform(X_test_scaled[feature_cols])\n",
    "    print(\"Imputation complete.\")\n",
    "    \n",
    "    return (X_train_imputed, y_train), (X_val_imputed, y_val), (X_test_imputed, y_test), scaler, imputer\n",
    "\n",
    "def augment_training_data(X_train_imputed, y_train):\n",
    "    \"\"\"\n",
    "    Applies the hybrid SMOTE and CTGAN augmentation strategy to the now-complete training set.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Augmenting Training Data ---\")\n",
    "    \n",
    "    id_cols = [col for col in CONFIG[\"ID_COLUMNS\"] if col in X_train_imputed.columns]\n",
    "    X_train_features = X_train_imputed.drop(columns=id_cols)\n",
    "    \n",
    "    train_df_features = pd.concat([X_train_features, y_train.reset_index(drop=True)], axis=1)\n",
    "    minority_df = train_df_features[train_df_features[CONFIG[\"TARGET_COLUMN\"]] == 1]\n",
    "    majority_df = train_df_features[train_df_features[CONFIG[\"TARGET_COLUMN\"]] == 0]\n",
    "    \n",
    "    num_to_generate = len(majority_df) - len(minority_df)\n",
    "    num_from_smote = num_to_generate // 2\n",
    "    num_from_gan = num_to_generate - num_from_smote\n",
    "\n",
    "    # --- SMOTE ---\n",
    "    print(\"\\nApplying SMOTE...\")\n",
    "    smote = SMOTE(sampling_strategy={1: len(minority_df) + num_from_smote}, random_state=CONFIG[\"RANDOM_STATE\"])\n",
    "    X_smote_aug, y_smote_aug = smote.fit_resample(X_train_features, y_train)\n",
    "    \n",
    "    smote_generated_df = pd.concat([pd.DataFrame(X_smote_aug), pd.DataFrame(y_smote_aug)], axis=1).iloc[len(X_train_features):]\n",
    "    smote_generated_df.columns = train_df_features.columns\n",
    "\n",
    "    # --- CTGAN (with Metadata Detection Fix) ---\n",
    "    print(\"Detecting metadata for CTGAN...\")\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=minority_df)\n",
    "\n",
    "    print(\"Training CTGAN on original minority data...\")\n",
    "    # Pass the detected metadata object to the synthesizer\n",
    "    ctgan = CTGANSynthesizer(metadata, epochs=300, verbose=False)\n",
    "    ctgan.fit(minority_df)\n",
    "    gan_generated_df = ctgan.sample(num_rows=num_from_gan)\n",
    "    \n",
    "    # --- Combine ---\n",
    "    print(\"\\nCombining original and synthetic data...\")\n",
    "    final_train_df_features = pd.concat([majority_df, minority_df, smote_generated_df, gan_generated_df], ignore_index=True)\n",
    "    \n",
    "    final_train_df = final_train_df_features.sample(frac=1, random_state=CONFIG[\"RANDOM_STATE\"]).reset_index(drop=True)\n",
    "    \n",
    "    X_train_aug = final_train_df.drop(columns=[CONFIG[\"TARGET_COLUMN\"]])\n",
    "    y_train_aug = final_train_df[CONFIG[\"TARGET_COLUMN\"]]\n",
    "    \n",
    "    print(f\"\\nFinal augmented training set distribution:\\n{y_train_aug.value_counts(normalize=True)}\")\n",
    "    return X_train_aug, y_train_aug\n",
    "\n",
    "def save_artifacts(train_data, val_data, test_data, scaler, imputer, output_dir):\n",
    "    \"\"\"Saves all processed data splits and the scaler/imputer objects to disk.\"\"\"\n",
    "    print(\"\\n--- 3. Saving Artifacts ---\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    X_train.to_csv(output_dir / \"train_X.csv\", index=False)\n",
    "    y_train.to_csv(output_dir / \"train_y.csv\", index=False)\n",
    "    X_val.to_csv(output_dir / \"val_X.csv\", index=False)\n",
    "    y_val.to_csv(output_dir / \"val_y.csv\", index=False)\n",
    "    X_test.to_csv(output_dir / \"test_X.csv\", index=False)\n",
    "    y_test.to_csv(output_dir / \"test_y.csv\", index=False)\n",
    "    \n",
    "    joblib.dump(scaler, output_dir / \"scaler.joblib\")\n",
    "    joblib.dump(imputer, output_dir / \"imputer.joblib\")\n",
    "    \n",
    "    print(f\"Processed data saved to '{output_dir.resolve()}'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline for Phase 2.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(CONFIG[\"INPUT_FILE\"])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{CONFIG['INPUT_FILE']}'\")\n",
    "        return\n",
    "        \n",
    "    train_data, val_data, test_data, scaler, imputer = split_scale_impute_data(df, CONFIG)\n",
    "    X_train_aug, y_train_aug = augment_training_data(train_data[0], train_data[1])\n",
    "    \n",
    "    save_artifacts((X_train_aug, y_train_aug), val_data, test_data, scaler, imputer, CONFIG[\"OUTPUT_DIR\"])\n",
    "    \n",
    "    print(\"\\nPhase 2 processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_med_diag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
