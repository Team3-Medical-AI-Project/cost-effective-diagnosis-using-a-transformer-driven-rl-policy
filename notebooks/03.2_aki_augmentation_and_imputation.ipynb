{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a6cc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Splitting, Scaling, and Imputing AKI Data ---\n",
      "Scaling complete.\n",
      "Imputation complete.\n",
      "\n",
      "--- 2. Augmenting AKI Training Data ---\n",
      "Original training distribution:\n",
      "kdigo_aki\n",
      "1    0.748931\n",
      "0    0.251069\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Applying SMOTE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:64: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.19932743  0.45290586  0.12678921 ...  1.36603248  1.17036249\n",
      "  1.23558582]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:64: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.18474485  0.8440636   0.8440636  ... -1.18474485  0.8440636\n",
      " -1.18474485]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:65: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.71379918 -1.96035734  0.51812919 ...  0.38768253  1.36603248\n",
      " -0.32977409]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:65: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8440636   0.8440636  -1.18474485 ...  0.8440636  -1.18474485\n",
      " -1.18474485]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:66: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.45290586 -0.85156073 -1.89513401 ...  1.23558582 -0.1341041\n",
      " -0.06888078]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_24532\\669683006.py:66: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.8440636   0.8440636   0.8440636  ... -1.18474485  0.8440636\n",
      " -1.18474485]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning:\n",
      "\n",
      "`BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 14718 samples and 9826 samples are asked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPhase 2 processing for AKI complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 165\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    164\u001b[0m train_data, val_data, test_data, scaler, imputer \u001b[38;5;241m=\u001b[39m split_scale_impute_data(df, CONFIG)\n\u001b[1;32m--> 165\u001b[0m X_train_aug, y_train_aug \u001b[38;5;241m=\u001b[39m \u001b[43maugment_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m save_artifacts((X_train_aug, y_train_aug), val_data, test_data, scaler, imputer, CONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPhase 2 processing for AKI complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36maugment_training_data\u001b[1;34m(X_train_imputed, y_train)\u001b[0m\n\u001b[0;32m    103\u001b[0m explicit_strategy \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28mlen\u001b[39m(majority_df), \n\u001b[0;32m    105\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mlen\u001b[39m(minority_df) \u001b[38;5;241m+\u001b[39m num_from_smote\n\u001b[0;32m    106\u001b[0m }\n\u001b[0;32m    107\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39mexplicit_strategy, random_state\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRANDOM_STATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 108\u001b[0m X_smote_aug, y_smote_aug \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m smote_generated_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mDataFrame(X_smote_aug), pd\u001b[38;5;241m.\u001b[39mDataFrame(y_smote_aug)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlen\u001b[39m(X_train_features):]\n\u001b[0;32m    111\u001b[0m smote_generated_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m train_df_features\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\imblearn\\base.py:108\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_type\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\imblearn\\utils\\_validation.py:557\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28msorted\u001b[39m(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    554\u001b[0m     )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[1;32m--> 557\u001b[0m         \u001b[38;5;28msorted\u001b[39m(\u001b[43m_sampling_strategy_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    558\u001b[0m     )\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28msorted\u001b[39m(_sampling_strategy_list(sampling_strategy, y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    562\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\vamsi\\anaconda3\\envs\\rl_med_diag\\lib\\site-packages\\imblearn\\utils\\_validation.py:335\u001b[0m, in \u001b[0;36m_sampling_strategy_dict\u001b[1;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_sample, n_samples \u001b[38;5;129;01min\u001b[39;00m sampling_strategy\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m target_stats[class_sample]:\n\u001b[1;32m--> 335\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith over-sampling methods, the number\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of samples in a class should be greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or equal to the original number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Originally, there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_stats[class_sample]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples are asked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m             )\n\u001b[0;32m    342\u001b[0m         sampling_strategy_[class_sample] \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m-\u001b[39m target_stats[class_sample]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munder-sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 14718 samples and 9826 samples are asked."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 2: Data Splitting, Imputation, and Augmentation for AKI Cohort (v1.5)\n",
    "\n",
    "Description:\n",
    "This script takes the preprocessed AKI feature matrix from Phase 1 and prepares it \n",
    "for model training.\n",
    "1. Splits the data into training, validation, and test sets.\n",
    "2. Applies feature scaling.\n",
    "3. Imputes missing values using a SimpleImputer.\n",
    "4. Balances the imputed training set using a hybrid SMOTE and CTGAN strategy.\n",
    "\n",
    "Outputs:\n",
    "- All processed files will be saved in a new 'aki' subfolder.\n",
    "- scaler_aki.joblib & imputer_aki.joblib: Saved objects for the AKI pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration for AKI Dataset ---\n",
    "CONFIG = {\n",
    "    \"INPUT_FILE\": \"../data/preprocessed/aki_feature_matrix.csv\",\n",
    "    \"OUTPUT_DIR\": Path(\"../data/processed/aki/\"),\n",
    "    \"TARGET_COLUMN\": \"kdigo_aki\",\n",
    "    \"ID_COLUMNS\": ['subject_id', 'hadm_id', 'stay_id'],\n",
    "    \"TEST_SIZE\": 0.2,\n",
    "    \"VALIDATION_SIZE\": 0.1,\n",
    "    \"RANDOM_STATE\": 42\n",
    "}\n",
    "\n",
    "def split_scale_impute_data(df, config):\n",
    "    \"\"\"\n",
    "    Loads, splits, scales, and imputes the data.\n",
    "    The scaler and imputer are fit ONLY on the training data and saved.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Splitting, Scaling, and Imputing AKI Data ---\")\n",
    "    \n",
    "    X = df.drop(columns=[config[\"TARGET_COLUMN\"]])\n",
    "    y = df[config[\"TARGET_COLUMN\"]]\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=config[\"TEST_SIZE\"], random_state=config[\"RANDOM_STATE\"], stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=config[\"VALIDATION_SIZE\"], random_state=config[\"RANDOM_STATE\"], stratify=y_train_val\n",
    "    )\n",
    "    \n",
    "    id_cols = [col for col in config[\"ID_COLUMNS\"] if col in X_train.columns]\n",
    "    feature_cols = [col for col in X_train.columns if col not in id_cols]\n",
    "    \n",
    "    # --- Feature Scaling ---\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_val_scaled = X_val.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    X_train_scaled.loc[:, feature_cols] = scaler.fit_transform(X_train[feature_cols])\n",
    "    X_val_scaled.loc[:, feature_cols] = scaler.transform(X_val[feature_cols])\n",
    "    X_test_scaled.loc[:, feature_cols] = scaler.transform(X_test[feature_cols])\n",
    "    print(\"Scaling complete.\")\n",
    "    \n",
    "    # --- Imputation ---\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    \n",
    "    X_train_imputed = X_train_scaled.copy()\n",
    "    X_val_imputed = X_val_scaled.copy()\n",
    "    X_test_imputed = X_test_scaled.copy()\n",
    "    \n",
    "    X_train_imputed.loc[:, feature_cols] = imputer.fit_transform(X_train_scaled[feature_cols])\n",
    "    X_val_imputed.loc[:, feature_cols] = imputer.transform(X_val_scaled[feature_cols])\n",
    "    X_test_imputed.loc[:, feature_cols] = imputer.transform(X_test_scaled[feature_cols])\n",
    "    print(\"Imputation complete.\")\n",
    "    \n",
    "    return (X_train_imputed, y_train), (X_val_imputed, y_val), (X_test_imputed, y_test), scaler, imputer\n",
    "\n",
    "def augment_training_data(X_train_imputed, y_train):\n",
    "    \"\"\"\n",
    "    Applies the hybrid SMOTE and CTGAN augmentation strategy to the now-complete training set.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Augmenting AKI Training Data ---\")\n",
    "    \n",
    "    id_cols = [col for col in CONFIG[\"ID_COLUMNS\"] if col in X_train_imputed.columns]\n",
    "    X_train_features = X_train_imputed.drop(columns=id_cols)\n",
    "    \n",
    "    train_df_features = pd.concat([X_train_features, y_train.reset_index(drop=True)], axis=1)\n",
    "    minority_df = train_df_features[train_df_features[CONFIG[\"TARGET_COLUMN\"]] == 1]\n",
    "    majority_df = train_df_features[train_df_features[CONFIG[\"TARGET_COLUMN\"]] == 0]\n",
    "    \n",
    "    num_to_generate = len(majority_df) - len(minority_df)\n",
    "    num_from_smote = num_to_generate // 2\n",
    "    num_from_gan = num_to_generate - num_from_smote\n",
    "\n",
    "    # --- SMOTE ---\n",
    "    print(f\"Original training distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "    print(\"\\nApplying SMOTE...\")\n",
    "    explicit_strategy = {\n",
    "        0: len(majority_df), \n",
    "        1: len(minority_df) + num_from_smote\n",
    "    }\n",
    "    smote = SMOTE(sampling_strategy=explicit_strategy, random_state=CONFIG[\"RANDOM_STATE\"])\n",
    "    X_smote_aug, y_smote_aug = smote.fit_resample(X_train_features, y_train)\n",
    "    \n",
    "    smote_generated_df = pd.concat([pd.DataFrame(X_smote_aug), pd.DataFrame(y_smote_aug)], axis=1).iloc[len(X_train_features):]\n",
    "    smote_generated_df.columns = train_df_features.columns\n",
    "\n",
    "    # --- CTGAN ---\n",
    "    print(\"Detecting metadata for CTGAN...\")\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data=minority_df)\n",
    "\n",
    "    print(\"Training CTGAN on original minority data...\")\n",
    "    ctgan = CTGANSynthesizer(metadata, epochs=300, verbose=False)\n",
    "    ctgan.fit(minority_df)\n",
    "    gan_generated_df = ctgan.sample(num_rows=num_from_gan)\n",
    "    \n",
    "    # --- Combine ---\n",
    "    print(\"\\nCombining original and synthetic data...\")\n",
    "    final_train_df_features = pd.concat([majority_df, minority_df, smote_generated_df, gan_generated_df], ignore_index=True)\n",
    "    \n",
    "    final_train_df = final_train_df_features.sample(frac=1, random_state=CONFIG[\"RANDOM_STATE\"]).reset_index(drop=True)\n",
    "    \n",
    "    X_train_aug = final_train_df.drop(columns=[CONFIG[\"TARGET_COLUMN\"]])\n",
    "    y_train_aug = final_train_df[CONFIG[\"TARGET_COLUMN\"]]\n",
    "    \n",
    "    print(f\"\\nFinal augmented training set distribution:\\n{y_train_aug.value_counts(normalize=True)}\")\n",
    "    return X_train_aug, y_train_aug\n",
    "\n",
    "def save_artifacts(train_data, val_data, test_data, scaler, imputer, output_dir):\n",
    "    \"\"\"Saves all processed data splits and the scaler/imputer objects to disk.\"\"\"\n",
    "    print(\"\\n--- 3. Saving Artifacts ---\")\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    X_train, y_train = train_data\n",
    "    X_val, y_val = val_data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    X_train.to_csv(output_dir / \"train_X.csv\", index=False)\n",
    "    y_train.to_csv(output_dir / \"train_y.csv\", index=False)\n",
    "    X_val.to_csv(output_dir / \"val_X.csv\", index=False)\n",
    "    y_val.to_csv(output_dir / \"val_y.csv\", index=False)\n",
    "    X_test.to_csv(output_dir / \"test_X.csv\", index=False)\n",
    "    y_test.to_csv(output_dir / \"test_y.csv\", index=False)\n",
    "    \n",
    "    joblib.dump(scaler, output_dir / \"scaler_aki.joblib\")\n",
    "    joblib.dump(imputer, output_dir / \"imputer_aki.joblib\")\n",
    "    \n",
    "    print(f\"Processed AKI data saved to '{output_dir.resolve()}'\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline for Phase 2 for the AKI cohort.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(CONFIG[\"INPUT_FILE\"])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{CONFIG['INPUT_FILE']}'\")\n",
    "        return\n",
    "        \n",
    "    train_data, val_data, test_data, scaler, imputer = split_scale_impute_data(df, CONFIG)\n",
    "    X_train_aug, y_train_aug = augment_training_data(train_data[0], train_data[1])\n",
    "    \n",
    "    save_artifacts((X_train_aug, y_train_aug), val_data, test_data, scaler, imputer, CONFIG[\"OUTPUT_DIR\"])\n",
    "    \n",
    "    print(\"\\nPhase 2 processing for AKI complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_med_diag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
